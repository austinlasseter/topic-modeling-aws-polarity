{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::443501626368:role/service-role/AmazonSageMaker-ExecutionRole-20200806T142735\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = sess.default_bucket() # Replace with your own bucket name if needed\n",
    "\n",
    "prefix = 'review_topics' #Replace with the prefix under which you want to store the data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-443501626368\n"
     ]
    }
   ],
   "source": [
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple tokenizer (NLTK won't be available to us later on, in our Lambda function)\n",
    "\n",
    "def simple_tokenizer(input_text):\n",
    "    REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(=)|(`)\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)|(\\n)|(\\t)\")\n",
    "    tokens = REPLACE_NO_SPACE.sub(\"\", input_text.lower())\n",
    "    tokens = REPLACE_WITH_SPACE.sub(\" \", tokens) # note that blazing text expects space-separated tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and process the dataset\n",
    "def preprocesser(input_file, output_file, output_csv):\n",
    "    \n",
    "    # read in the dataset\n",
    "    filepath=Path.joinpath(Path.cwd().parent, 'data', 'amazon_review_polarity_csv', input_file)\n",
    "    df = pd.read_csv(filepath, names=[\"Label\", \"Title\", \"Review\"])\n",
    "    df = df.sample(frac = 0.35)\n",
    "    # class 0 is the negative and class 1 is the positive\n",
    "    df['Label']=df['Label']-1\n",
    "    \n",
    "    # drop Title, focus on review itself\n",
    "    df=df.drop('Title', axis=1)\n",
    "    \n",
    "    # Shuffle reviews and corresponding labels within training and test sets\n",
    "    df = df.sample(frac = 1)\n",
    "    \n",
    "    # apply the simple tokenizer\n",
    "    df['Review']=df['Review'].apply(simple_tokenizer)\n",
    "    \n",
    "    # Prefix the index-ed label with __label__\n",
    "    df['Label']=df['Label'].apply(lambda row: \"__label__\" +  str(row) )\n",
    "    \n",
    "    # convert the transformed dataframe into a list\n",
    "    transformed_rows = np.array(df).tolist()\n",
    "    \n",
    "    # write to csv file (for blazingtext)\n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n') # notice the delimiter.\n",
    "        csv_writer.writerows(transformed_rows)\n",
    "        \n",
    "    # write to csv file (for LDA topic modeling)\n",
    "    df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 50s, sys: 2.03 s, total: 3min 52s\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Preparing the training dataset\n",
    "preprocesser('train.csv', 'polarity.train', 'train_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 s, sys: 164 ms, total: 17.8 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Preparing the validation dataset        \n",
    "preprocesser('test.csv', 'polarity.validation',  'test_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing cell might take a minute to run. After the data preprocessing is complete, we need to upload it to S3 so that it can be consumed by SageMaker to execute training jobs. We'll use Python SDK to upload these two files to the bucket and prefix location that we have set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.08 s, sys: 909 ms, total: 7.99 s\n",
      "Wall time: 5.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='polarity.train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='polarity.validation', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Sentiment Classifier\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the SageMaker `Estimator` with resource configurations and hyperparameters to train Text Classification on *DBPedia* dataset, using \"supervised\" mode on a `c4.4xlarge` instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html) for the complete list of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyper-parameters are setup, let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our data channels. These objects are then put in a simple dictionary, which the algorithm consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the algorithm involves a few steps. Firstly, the instance that we requested while creating the Estimator classes is provisioned and is setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 16:48:57 Starting - Starting the training job...\n",
      "2020-08-13 16:49:02 Starting - Launching requested ML instances.........\n",
      "2020-08-13 16:50:43 Starting - Preparing the instances for training...\n",
      "2020-08-13 16:51:27 Downloading - Downloading input data...\n",
      "2020-08-13 16:51:49 Training - Downloading the training image...\n",
      "2020-08-13 16:52:06 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[08/13/2020 16:52:07 WARNING 140211784693568] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[08/13/2020 16:52:07 WARNING 140211784693568] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[08/13/2020 16:52:07 INFO 140211784693568] nvidia-smi took: 0.0252530574799 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[08/13/2020 16:52:07 INFO 140211784693568] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34m[08/13/2020 16:52:07 INFO 140211784693568] Processing /opt/ml/input/data/train/polarity.train . File size: 349 MB\u001b[0m\n",
      "\u001b[34m[08/13/2020 16:52:07 INFO 140211784693568] Processing /opt/ml/input/data/validation/polarity.validation . File size: 38 MB\u001b[0m\n",
      "\u001b[34mRead 10M words\u001b[0m\n",
      "\u001b[34mRead 20M words\u001b[0m\n",
      "\u001b[34mRead 30M words\u001b[0m\n",
      "\u001b[34mRead 40M words\u001b[0m\n",
      "\u001b[34mRead 50M words\u001b[0m\n",
      "\u001b[34mRead 60M words\u001b[0m\n",
      "\u001b[34mRead 68M words\u001b[0m\n",
      "\u001b[34mNumber of words:  264662\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/polarity.validation\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0451  Progress: 9.84%  Million Words/sec: 36.58 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0426  Progress: 14.90%  Million Words/sec: 37.24 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0400  Progress: 20.00%  Million Words/sec: 37.66 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0375  Progress: 25.10%  Million Words/sec: 37.91 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0349  Progress: 30.17%  Million Words/sec: 38.05 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 3\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0324  Progress: 35.28%  Million Words/sec: 38.18 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0298  Progress: 40.38%  Million Words/sec: 38.28 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0273  Progress: 45.49%  Million Words/sec: 38.35 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.90884\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0247  Progress: 50.66%  Million Words/sec: 37.46 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0221  Progress: 55.76%  Million Words/sec: 37.59 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.91346\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0195  Progress: 60.98%  Million Words/sec: 36.96 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0170  Progress: 66.09%  Million Words/sec: 37.11 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.91292\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0144  Progress: 71.11%  Million Words/sec: 36.73 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0119  Progress: 76.21%  Million Words/sec: 36.87 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.91351\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0093  Progress: 81.50%  Million Words/sec: 36.45 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0067  Progress: 86.59%  Million Words/sec: 36.58 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.91408\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0041  Progress: 91.73%  Million Words/sec: 36.21 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0016  Progress: 96.76%  Million Words/sec: 36.31 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.91505\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 36.05 #####\u001b[0m\n",
      "\n",
      "2020-08-13 16:52:53 Uploading - Uploading generated training model\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 36.05\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 19.08\n",
      "\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.9856\u001b[0m\n",
      "\u001b[34mNumber of train examples: 900000\n",
      "\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.9151\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 100000\u001b[0m\n",
      "\n",
      "2020-08-13 16:53:10 Completed - Training job completed\n",
      "Training seconds: 103\n",
      "Billable seconds: 103\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting / Inference\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. \n",
    "note on pricing: https://aws.amazon.com/sagemaker/pricing/instance-types/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlazingText supports `application/json` as the content-type for inference. The payload should contain a list of sentences with the key as \"**instances**\" while being passed to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      1.0000100135803223\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__1\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentences = ['best happy joy smile wonderful amazing loved it recommend very glad excited eager feeling good quality top-notch great']\n",
    "tokenized_sentences = [simple_tokenizer(sentences[0])]\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "response = text_classifier.predict(json.dumps(payload)) \n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for Lambda Function\n",
    "When we eventually construct our Lambda function we won't have access to the `text_classifier` object, so how do we call a SageMaker endpoint?\n",
    "\n",
    "Python functions that are used in Lambda have access to another Amazon library called boto3. The `boto3` library provides an API for working with Amazon services, including SageMaker. To start with, we need to get a handle to the SageMaker runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<botocore.client.SageMakerRuntime object at 0x7f9c996ea550>\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "runtime = boto3.Session().client('sagemaker-runtime')\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that we have access to the SageMaker runtime, we can ask it to make use of (invoke) an endpoint that has already been created. However, we need to provide SageMaker with the name of the deployed endpoint. To find this out we can print it out using the text_classifier object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blazingtext-2020-08-13-16-48-57-681\n"
     ]
    }
   ],
   "source": [
    "print(text_classifier.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function to call the endpoint\n",
    "def make_prediction(sentences):\n",
    "    sentences=[sentences]\n",
    "    tokenized_sentences = [simple_tokenizer(sentences[0])]\n",
    "    payload = {\"instances\" : tokenized_sentences}\n",
    "    response = text_classifier.predict(json.dumps(payload)) \n",
    "\n",
    "    response = runtime.invoke_endpoint(EndpointName = text_classifier.endpoint, # The name of the endpoint we created\n",
    "                                           ContentType = 'application/json', # The data format that is expected\n",
    "                                           Body = json.dumps(payload))\n",
    "\n",
    "    output = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    print(output[0]['prob'][0])\n",
    "    print(output[0]['label'][0].split('__label__')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000100135803223\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# try a positive sentence\n",
    "\n",
    "make_prediction('best happy joy smile wonderful amazing loved it recommend very glad excited eager feeling good quality top-notch great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000100135803223\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# try a negative sentence\n",
    "\n",
    "make_prediction('awful horrible hated worst never disgusting bad terrible destroy bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
