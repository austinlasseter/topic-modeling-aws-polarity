{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA topic modeling\n",
    "#### Example using the 20 Newsgroups dataset from sklearn\n",
    "* https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n",
    "* https://medium.com/@yanlinc/how-to-build-a-lda-topic-model-using-from-text-601cdcbfd3a6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple tokenizer (NLTK won't be available to us later on, in our Lambda function)\n",
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(=)|(`)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)|(\\n)|(\\t)\")\n",
    "\n",
    "def simple_tokenizer(input_text):\n",
    "    tokens = REPLACE_NO_SPACE.sub(\"\", input_text.lower())\n",
    "    tokens = REPLACE_WITH_SPACE.sub(\" \", tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the text\n",
    "tok_documents = [simple_tokenizer(doc) for doc in documents]\n",
    "len(tok_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well im not sure about the story nad it did seem biased what i disagree with is your statement that the us media is out to ruin israels reputation that is rediculous the us media is the most pro israeli media in the world having lived in europe i realize that incidences such as the one described in the letter have occured the us media as a whole seem to try to ignore them the us is subsidizing israels existance and the europeans are not at least not to the same degree so i think that might be a reason they report more clearly on the atrocities  what is a shame is that in austria daily reports of the inhuman acts commited by israeli soldiers and the blessing received from the government makes some of the holocaust guilt go away after all look how the jews are treating other races when they got power it is unfortunate '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text with TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(tok_documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=50.0,\n",
       "                          max_doc_update_iter=100, max_iter=5,\n",
       "                          mean_change_tol=0.001, n_components=20, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Instantiate the LDA class object from scikitlearn\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, \n",
    "                                max_iter=5, \n",
    "                                learning_method='online', \n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "# Fit our LDA model onto the vectorized text data\n",
    "lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names\n",
    "topicnames = ['Topic' + str(i) for i in range(lda.n_components)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>02</th>\n",
       "      <th>04</th>\n",
       "      <th>0t</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>x11</th>\n",
       "      <th>xt</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic0</th>\n",
       "      <td>0.050064</td>\n",
       "      <td>0.050020</td>\n",
       "      <td>0.050030</td>\n",
       "      <td>0.050017</td>\n",
       "      <td>0.050484</td>\n",
       "      <td>0.050024</td>\n",
       "      <td>1.134016</td>\n",
       "      <td>0.050022</td>\n",
       "      <td>0.050070</td>\n",
       "      <td>0.050123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>0.050017</td>\n",
       "      <td>0.050034</td>\n",
       "      <td>0.166502</td>\n",
       "      <td>0.050538</td>\n",
       "      <td>0.050521</td>\n",
       "      <td>0.050024</td>\n",
       "      <td>0.050085</td>\n",
       "      <td>0.050032</td>\n",
       "      <td>0.050042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>0.704079</td>\n",
       "      <td>1.422826</td>\n",
       "      <td>1.709286</td>\n",
       "      <td>0.050020</td>\n",
       "      <td>19.092435</td>\n",
       "      <td>9.271399</td>\n",
       "      <td>3.652171</td>\n",
       "      <td>8.272621</td>\n",
       "      <td>12.355965</td>\n",
       "      <td>8.647210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050019</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>64.334747</td>\n",
       "      <td>79.356512</td>\n",
       "      <td>42.112647</td>\n",
       "      <td>16.139716</td>\n",
       "      <td>10.725409</td>\n",
       "      <td>22.960021</td>\n",
       "      <td>49.971997</td>\n",
       "      <td>18.013730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>0.050020</td>\n",
       "      <td>0.050016</td>\n",
       "      <td>0.050020</td>\n",
       "      <td>0.050022</td>\n",
       "      <td>0.050019</td>\n",
       "      <td>0.050018</td>\n",
       "      <td>0.050023</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>0.050019</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050020</td>\n",
       "      <td>0.050018</td>\n",
       "      <td>0.050019</td>\n",
       "      <td>0.050070</td>\n",
       "      <td>0.050020</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>0.050020</td>\n",
       "      <td>0.050048</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>0.050016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>0.050020</td>\n",
       "      <td>0.050018</td>\n",
       "      <td>0.050020</td>\n",
       "      <td>0.050022</td>\n",
       "      <td>0.050019</td>\n",
       "      <td>0.050018</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>0.050018</td>\n",
       "      <td>0.050026</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050017</td>\n",
       "      <td>0.050018</td>\n",
       "      <td>0.050028</td>\n",
       "      <td>0.050034</td>\n",
       "      <td>0.050072</td>\n",
       "      <td>0.050019</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>0.050034</td>\n",
       "      <td>0.050063</td>\n",
       "      <td>0.050020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>9.231740</td>\n",
       "      <td>6.486786</td>\n",
       "      <td>3.931163</td>\n",
       "      <td>0.144278</td>\n",
       "      <td>10.494104</td>\n",
       "      <td>5.501869</td>\n",
       "      <td>2.826167</td>\n",
       "      <td>5.728174</td>\n",
       "      <td>7.733292</td>\n",
       "      <td>3.742519</td>\n",
       "      <td>...</td>\n",
       "      <td>12.511049</td>\n",
       "      <td>9.827182</td>\n",
       "      <td>5.553785</td>\n",
       "      <td>2.626648</td>\n",
       "      <td>9.272604</td>\n",
       "      <td>1.292217</td>\n",
       "      <td>7.256000</td>\n",
       "      <td>0.321680</td>\n",
       "      <td>8.146721</td>\n",
       "      <td>1.310245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              00        02        04        0t         10       100      1000  \\\n",
       "Topic0  0.050064  0.050020  0.050030  0.050017   0.050484  0.050024  1.134016   \n",
       "Topic1  0.704079  1.422826  1.709286  0.050020  19.092435  9.271399  3.652171   \n",
       "Topic2  0.050020  0.050016  0.050020  0.050022   0.050019  0.050018  0.050023   \n",
       "Topic3  0.050020  0.050018  0.050020  0.050022   0.050019  0.050018  0.050021   \n",
       "Topic4  9.231740  6.486786  3.931163  0.144278  10.494104  5.501869  2.826167   \n",
       "\n",
       "              11         12        13  ...        x11        xt       year  \\\n",
       "Topic0  0.050022   0.050070  0.050123  ...   0.050021  0.050017   0.050034   \n",
       "Topic1  8.272621  12.355965  8.647210  ...   0.050019  0.050021  64.334747   \n",
       "Topic2  0.050021   0.050019  0.050021  ...   0.050020  0.050018   0.050019   \n",
       "Topic3  0.050018   0.050026  0.050021  ...   0.050017  0.050018   0.050028   \n",
       "Topic4  5.728174   7.733292  3.742519  ...  12.511049  9.827182   5.553785   \n",
       "\n",
       "            years        yes       york      youll      young      youre  \\\n",
       "Topic0   0.166502   0.050538   0.050521   0.050024   0.050085   0.050032   \n",
       "Topic1  79.356512  42.112647  16.139716  10.725409  22.960021  49.971997   \n",
       "Topic2   0.050070   0.050020   0.050021   0.050020   0.050048   0.050021   \n",
       "Topic3   0.050034   0.050072   0.050019   0.050021   0.050034   0.050063   \n",
       "Topic4   2.626648   9.272604   1.292217   7.256000   0.321680   8.146721   \n",
       "\n",
       "            youve  \n",
       "Topic0   0.050042  \n",
       "Topic1  18.013730  \n",
       "Topic2   0.050016  \n",
       "Topic3   0.050020  \n",
       "Topic4   1.310245  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic-Keyword Matrix\n",
    "import pandas as pd\n",
    "df_topic_keywords = pd.DataFrame(lda.components_)\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = tfidf_vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "# View\n",
    "df_topic_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 15 keywords for each topic\n",
    "import numpy as np\n",
    "\n",
    "def show_topics(vectorizer, lda_model, n_words):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 15 keywords each topic\n",
    "topic_keywords = show_topics(vectorizer=tfidf_vectorizer, lda_model=lda, n_words=15)\n",
    "len(topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>israel</td>\n",
       "      <td>israeli</td>\n",
       "      <td>jews</td>\n",
       "      <td>armenian</td>\n",
       "      <td>armenians</td>\n",
       "      <td>turkish</td>\n",
       "      <td>turkey</td>\n",
       "      <td>war</td>\n",
       "      <td>killed</td>\n",
       "      <td>turks</td>\n",
       "      <td>peace</td>\n",
       "      <td>jewish</td>\n",
       "      <td>armenia</td>\n",
       "      <td>land</td>\n",
       "      <td>soviet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>people</td>\n",
       "      <td>dont</td>\n",
       "      <td>think</td>\n",
       "      <td>just</td>\n",
       "      <td>god</td>\n",
       "      <td>like</td>\n",
       "      <td>know</td>\n",
       "      <td>time</td>\n",
       "      <td>did</td>\n",
       "      <td>good</td>\n",
       "      <td>say</td>\n",
       "      <td>said</td>\n",
       "      <td>im</td>\n",
       "      <td>right</td>\n",
       "      <td>does</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word 0   Word 1 Word 2    Word 3     Word 4   Word 5  Word 6 Word 7  \\\n",
       "Topic 0  israel  israeli   jews  armenian  armenians  turkish  turkey    war   \n",
       "Topic 1  people     dont  think      just        god     like    know   time   \n",
       "\n",
       "         Word 8 Word 9 Word 10 Word 11  Word 12 Word 13 Word 14  \n",
       "Topic 0  killed  turks   peace  jewish  armenia    land  soviet  \n",
       "Topic 1     did   good     say    said       im   right    does  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "# Extract topic probability scores with LDA Transform\n",
    "topic_probability_scores = lda.transform(tfidf)\n",
    "print(len(topic_probability_scores))\n",
    "print(len(tok_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_toks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-34442d9c187d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdom_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_probability_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_probability_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred_topic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdom_topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_toks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred_topic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf_topic_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_toks' is not defined"
     ]
    }
   ],
   "source": [
    "# create a dataframe with our results\n",
    "df_final = pd.DataFrame(documents, columns=['text'])\n",
    "dom_topics=[np.argmax(topic_probability_scores[index]) for index in range(len(topic_probability_scores))]\n",
    "df_final['pred_topic']=dom_topics\n",
    "df_final['topic_words']=df_toks['pred_topic'].apply(lambda x: df_topic_keywords.iloc[x].values.tolist())\n",
    "df_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.iloc[9][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.iloc[9][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
